---
title: dropout正则化
date: 2018-04-06 11:24:33
tags: deeplearning
---
# 理解dropout
* 用于解决过拟合问题
* Dropout存在两个版本：直接（不常用）和反转。(这里只对Inverted Dropout进行说明)
* dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。（注意是暂时）

![](https://upload-images.jianshu.io/upload_images/10461798-b6b4afc39e214094.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图为Dropout的可视化表示，左边是应用Dropout之前的网络，右边是应用了Dropout的同一个网络。
## dropout 如何工作
我们知道，典型的神经网络其训练流程是将输入通过网络进行正向传导，然后将误差进行反向传播。Dropout就是针对这一过程之中，随机地删除隐藏层的部分单元，进行上述过程。
综合而言，上述过程可以分步骤为：
* 随机删除网络中的一些隐藏神经元，保持输入输出神经元不变
* 将输入通过修改后的网络进行前向传播，然后将误差通过修改后的网络进行反向传播
* 对于另外一批的训练样本，重复上述操作

在训练阶段期间对激活值进行缩放，而测试阶段保持不变
下面是对一层神经网络的实施dropout代码实现：
```python
d = np.random.rand(3, 2) < keep_drop
a = np.multiply(a, d)
a /= keep_drop //对激活值进行缩放，为了不影响下一层网络的最后的预期输出值，加上这个步骤来修正损失的值
```
## dropout为何有效
* 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况，减少神经元之间复杂的共适应性。
* 由于每一次都会随机地删除节点，下一个节点的输出不再那么依靠上一个节点，也就是说它在分配权重时，不会给上一层的某一结点非配过多的权重，起到了和L2正则化压缩权重差不多的作用。
* 可以将dropout看作是模型平均的一种，平均一个大量不同的网络。不同的网络在不同的情况下过拟合，虽然不同的网络可能会产生不同程度的过拟合，但是将其公用一个损失函数，相当于对其同时进行了优化，取了平均，因此可以较为有效地防止过拟合的发生。对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时共享隐含节点的权值，这种平均的架构被发现通常是十分有用的来减少过拟合方法。

## dropout使用技巧
* 在可能出现过拟合的网络层使用dropout
* dropout也可以被用作一种添加噪声的方法，直接对input进行操作。输入层设为更接近1的数，使得输入变化不会太大

## dropout缺点
* 明确定义的损失函数每一次迭代都会下降，而dropout每一次都会随机删除节点，也就是说每一次训练的网络都是不同的，损失函数不再被明确地定义，在某种程度上很难计算，我们失去了调试工具。

# 当前Dropout的使用情况
当前Dropout被大量利用于全连接网络，而且一般人为设置为0.5或者0.3，而在卷积隐藏层由于卷积自身的稀疏化以及稀疏化的ReLu函数的大量使用等原因，Dropout策略在卷积隐藏层中使用较少。
总体而言，Dropout是一个超参，需要根据具体的网路，具体的应用领域进行尝试。

# 参考资料
* [deeplearning.ai](https://www.deeplearning.ai/)
注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。