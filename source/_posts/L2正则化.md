---
title: L2正则化
date: 2018-04-05 13:56:49
tags: deeplearning
---
# 介绍
如果神经网络过度拟合了数据，也就是存在高方差问题。我们经常会使用正则化来降低方差，也可以准备更多的数据，但是有时无法获取到足够的数据或者说获取数据的成本很高，所以正则化是一个很好的方法来解决过拟合问题。
# 实现
## 公式
![L2正则化](https://upload-images.jianshu.io/upload_images/10461798-773ce1f32c861958.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
## 解释
* λ 是正则化参数，通常使用验证集或者交叉验证集来配置这个参数
* 其中第二项为正则化项： ![正则化项](https://upload-images.jianshu.io/upload_images/10461798-80c7d439a9b23729.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 加上正则化项后如何实现实现梯度下降
反向传播时：![](https://upload-images.jianshu.io/upload_images/10461798-e814314464053e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
更新参数：![](https://upload-images.jianshu.io/upload_images/10461798-7a3ba73006488b09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
事实上，这相当于我们把矩阵W减去αλ/m倍的它，避免权重矩阵过大，这也时L2正则化有时被叫做权重衰减的原因
## 为什么正则化可以降低方差
### 直观上理解
如果正则化λ设置得足够大，权重矩阵W将被设置为接近于0的值，也就是把多个隐藏单元的权重设为0，消除了这些隐藏单元的许多影响。
这个大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，使过拟合的网络变成更接近高偏差的状态，但是λ会存在一个中间值，使得这个网络维持在一个低方差且偏差不那么高的状态。
### 下面以tanh为激活函数为例子
![](https://upload-images.jianshu.io/upload_images/10461798-e38f8618c6c52373.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
当λ参数变大时，W矩阵变小，相应地Z矩阵也会变小，使得它维持在中间红色的部分，即相对线性的部分，使得网络更加线性，表达能力减弱，从而实现减低方差的功能
### 问题
* 为什么只正则化参数w，不加上参数b？
因为w通常是一个高维参数矢量，已经可以表达高方差的问题

### 注意事项
* 当观察损失函数变化形势时，应使用之前定义的有正则化项的损失函数，否则可能观察不到单调递减的现象，因为此时损失函数已经有了全新的定义。

### 参考资料
* [deeplearning.ai](https://www.deeplearning.ai/)
注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。